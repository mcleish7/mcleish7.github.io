---
title: "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach"
collection: publications
permalink: /publications/2025-02-10-Scaling
excerpt: 'We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.'
date: 2025-02-10
venue: 'NeurIPS (2025)'
paperurl: 'https://arxiv.org/abs/2502.05171'
citation: 'Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele and Tom Goldstein, Geiping (2025). &quot;Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach.&quot; <i>arXiv preprint arXiv:2502.05171</i>.'
authors: 'Jonas Geiping, <b>Sean McLeish</b>, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele and Tom Goldstein'
---
We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.

[Download paper here](https://arxiv.org/abs/2502.05171)

[GitHub Code](https://github.com/seal-rg/recurrent-pretraining)

[Hugging Face Models](https://huggingface.co/tomg-group-umd/huginn-0125)